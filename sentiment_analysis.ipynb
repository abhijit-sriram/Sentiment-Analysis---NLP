{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTIMENT ANALYSIS USING PYTORCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code we will be building a machine learning model to detect the sentiment (i.e., if a sentence is positive or negative) based on movie reviews provided by a user using PyTorch. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext.legacy.data import Field\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameters of a Field specify how the data should be processed.\n",
    "# We use the TEXT field to define how the review should be processed, and the LABEL field to process the sentiment.\n",
    "TEXT = data.Field(tokenize = 'spacy',tokenizer_language = 'en_core_web_sm')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature of TorchText is that it has support for common datasets used in natural language processing (NLP)\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data= train_data.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size=25000\n",
    "TEXT.build_vocab(train_data,max_size= max_vocab_size)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n",
      "unique tokens in Text vocabulary :25002\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "print(f\"unique tokens in Text vocabulary :{len(TEXT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 202478), (',', 192130), ('.', 165407), ('a', 109230), ('and', 109175), ('of', 101087), ('to', 93504), ('is', 76398), ('in', 61293), ('I', 54008), ('it', 53329), ('that', 48904), ('\"', 44045), (\"'s\", 43248), ('this', 42372), ('-', 37498), ('/><br', 35684), ('was', 34978), ('as', 30125), ('with', 29740), ('for', 29560), ('movie', 29474), ('film', 27275), ('The', 26165), ('but', 24639), ('on', 23054), (\"n't\", 23008), ('(', 22872), (')', 22478), ('you', 21270), ('are', 20938), ('not', 20339), ('have', 19743), ('his', 19552), ('be', 18729), ('he', 17391), ('one', 17026), ('!', 15468), ('by', 15379), ('at', 15163)]\n",
      "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is', 'in', 'I', 'it', 'that', '\"', \"'s\", 'this', '-', '/><br', 'was', 'as', 'with', 'for', 'movie', 'film', 'The', 'but', 'on', \"n't\", '(']\n",
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(40))\n",
    "print(TEXT.vocab.itos[:30])\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a GPU is available, use it if available, otherwise use the CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create data iterators for the training, validation, and test sets\n",
    "# Set the batch size and device for the iterators\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom RNN class that inherits from nn.Module\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Define an Embedding layer\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        # Define an RNN layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Define a Linear layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # Embed the input text\n",
    "        embedded_text = self.embedding(text)\n",
    "        \n",
    "        # Pass the embedded text through the RNN layer\n",
    "        rnn_output, hidden_state = self.rnn(embedded_text)\n",
    "        \n",
    "        # Assert that the last output of the RNN is equal to the final hidden state\n",
    "        assert torch.equal(rnn_output[-1,:,:], hidden_state.squeeze(0))\n",
    "        \n",
    "        # Pass the final hidden state through the Linear layer\n",
    "        output = self.fc(hidden_state.squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Counts the number of trainable parameters in a PyTorch model.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent optimizer with a learning rate of 0.001\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross Entropy with Logits loss function for binary classification\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "# The function binary_accuracy calculates the accuracy of the model's predictions, given the predicted values and the actual labels. It takes two arguments:\n",
    "# preds: the predicted values of the model.\n",
    "# y: the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Trains the given model on the given data iterator, using the given optimizer and loss criterion.\n",
    "    \"\"\"\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize epoch loss and accuracy\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "\n",
    "    # Iterate over the batches in the iterator\n",
    "    for i, batch in enumerate(iterator):\n",
    "        # Zero the gradients of the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the model predictions for the current batch\n",
    "        text = batch.text\n",
    "        label = batch.label\n",
    "        preds = model(text).squeeze(1)\n",
    "\n",
    "        # Compute the loss and accuracy for the current batch\n",
    "        loss = criterion(preds, label)\n",
    "        acc = binary_accuracy(preds, label)\n",
    "\n",
    "        # Backpropagate the loss and update the model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss and accuracy for the epoch\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    # Compute the average loss and accuracy for the epoch\n",
    "    num_batches = len(iterator)\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    avg_acc = epoch_acc / num_batches\n",
    "\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize loss and accuracy\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Loop over batches in iterator\n",
    "        for batch in iterator:\n",
    "            # Make predictions using the model\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            # Compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            # Accumulate loss and accuracy across batches\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    # Compute and return average loss and accuracy across batches\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model parameters saved in 'tut1-model.pt'\n",
    "model.load_state_dict(torch.load('tut1-model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = evaluate(model, test_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 47.22%\n",
      "Test Loss: 0.711%\n"
     ]
    }
   ],
   "source": [
    "print(f'Test Acc: {acc*100:.2f}%')\n",
    "print(f'Test Loss: {loss:.3f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
